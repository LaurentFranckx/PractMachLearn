---
output: html_document
---
#Introduction

Traditionally, human activity recognition research (HAR) has focused on discriminating between different activities. The
current analysis is based on a dataset which has been constructed with the purpose to investigate "how (well)" a weight lifting exercise was performed by the wearer.

As explained in the documentation to the dataset, 6 young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. 

#Initial data analysis 

Both the training and the test data have been obtained directly from the course website. 

```{r, echo=FALSE , warning=FALSE, message=FALSE}
library(knitr)
library(caret)
library(gbm)

setwd("U:\\Document\\practmachinelearningproj")
set.seed(123)

downloadfiles <- FALSE

if(downloadfiles == TRUE){
  url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(url_train, destfile = "train.csv")
  
  url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(url_test, destfile = "test.csv")
}


train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

The  variable *`r  names(train)[!(names(train) %in% names(test))] `* is included in the training data set but not in the test data set, while  variable *`r  names(test)[!(names(test) %in% names(train))] `* is included in the test data set but not in the training data set. Thus, the test data cannot be used to verify the out of sample error, and we will need to create a separate validation data set - see further).


As shown below, the training datatset is distributed more or less proportionally over the five Classes - there are thus no issues of class unbalance.

```{r, echo=FALSE , warning=FALSE, message=FALSE}
#no issue of unbalanced classes
summary(train$classe)
```



```{r, echo=FALSE , warning=FALSE, message=FALSE}
#eliminate variables with litlle or now variation
nsv <- nearZeroVar(train)
#length(nsv)
train <- train[  , -nsv]
test <- test[  , -nsv]
```

However, it turns out that there are `r length(nsv) ` variables with near zero variance in the training data set. These variables are eliminated from both the training and the test dataset. 


```{r, echo=FALSE , warning=FALSE, message=FALSE}
#elminate all variables where more than 96% of values are NA
na_count <- apply(train, 2, function(x) sum(is.na(x)))
na_count_prop <- apply(train, 2, function(x) sum(is.na(x))/length(x))
na_count_prop_low <- na_count_prop[ na_count_prop < 0.96]

train <- train[  , names(na_count_prop_low)]
train <- train[  , setdiff(names(train),c("X", "user_name", "raw_timestamp_part_1"  , 
                                          "raw_timestamp_part_2", "cvtd_timestamp",	"num_window"))]

test <- test[  , union(setdiff(names(train), "classe"),"problem_id")]

#to do: verify this
#test <- test[  , setdiff(names(test),c("x", "user_name"))]

#nrow(train[complete.cases(train),])
```

Moreover, for `r length(na_count_prop_low) ` variables, the number of NA observations exceeds 96%. These variables have also been eliminated from the test and the train data set. After this correction, all NA values have been eliminated from the data.

Several variables ("X", "user_name", "raw_timestamp_part_1"  , 
"raw_timestamp_part_2", "cvtd_timestamp",  "num_window") identify the observation, but do not describe the physical activity linked to the actual weight lifting exercise. These have also been eliminated from both data sets.

The actual estimations are thus based on the observation of 
`r ncol(train) ` explanatory variables. 

As noted above, the existing test data set does not contain the actual "class" value. Therefore, in order to obtain an estimate of the out of sample error, we further divide the "train" data set in a "train" and in a "validation" data set:

```{r, echo=TRUE , warning=FALSE, message=FALSE}
inTrain <- createDataPartition(y = train$classe, p =0.8, list = FALSE)

train <- train[inTrain, ]
valid <- train[-inTrain, ]

```

#Estimates

As the dependent variable is categorical, the use of linear regression is not appropriate for this model. Moreover, as the number of categories exceeds two, the use of a binary logit model can also be excluded. 

```{r, echo=FALSE , warning=FALSE, message=FALSE}
#lda leads to very low accuracy 
# preProcPCA <- preProcess(train[  , - ncol(train)],method="pca", thresh = 0.95 )
# trainPC <- predict(preProcPCA,train[  , - ncol(train)])
# modelPCAGLM <- train(train$classe ~ .,method="lda",data=trainPC)
# predPCAGLM <- predict(modelPCAGLM,test)
modellda <- train(train$classe ~ .,method="lda",data=train)
#modellda
```

We have considered using linear discriminant analysis. While an accuracy of `r round(modellda$results$Accuracy,3)` is not a bad result, this performance  is much lower than that of the alternatives that are described below, and it will not be discussed further. 


##Boosting with trees

We have first considered fitting a "boosting with trees" model.



```{r, echo=FALSE , warning=FALSE, message=FALSE}
modFitgbmfirstrun <- FALSE

if(modFitgbmfirstrun == TRUE){
  modFitgbm <- train(classe  ~ .  , data = train, method="gbm",verbose = FALSE,
                          preProcess=c("center","scale")
                          )
  save(modFitgbm, file = "modFitgbm.RData")  
  } else {
    load( "modFitgbm.RData")  
  }
```


```{r, echo=FALSE , warning=FALSE, message=FALSE}
modFitgbm$finalModel

modFitgbm

optrowgbm <- modFitgbm$results[modFitgbm$results$n.trees == modFitgbm$bestTune$n.trees
& modFitgbm$results$interaction.depth == modFitgbm$bestTune$interaction.depth & modFitgbm$results$shrinkage == modFitgbm$bestTune$shrinkage, ]

optaccugbm <- optrowgbm[, "Accuracy"]
```


Initially, the model has been bootstrapped for the purpose of cross validation with `r optrowgbm$control$number  ` resamples. This leads to an in-sample accuracy of `r round(optaccugbm,3)` for the preferred model. We also see that increasing the number of iterations and the maximum tree depth have a positive influence on the accuracy of the estimates:


```{r, echo=FALSE , warning=FALSE, message=FALSE}
plot(modFitgbm)
```


We have repeated the same estimation method, but we used K-foll cross-validation, with K = 10.


```{r, echo=FALSE , warning=FALSE, message=FALSE}
modFitgbmcvfirstrun <- FALSE

if(modFitgbmcvfirstrun == TRUE){
  gbmctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)

  modFitgbmcv <- train(classe  ~ .  , data = train, method="gbm",trControl = gbmctrl, verbose = FALSE,
                          preProcess=c("center","scale")
                          )
  save(modFitgbmcv, file = "modFitgbmcv.RData")  
  } else {
    load( "modFitgbmcv.RData")  
  }


optrowgbmcv <- modFitgbmcv$results[modFitgbmcv$results$n.trees == modFitgbmcv$bestTune$n.trees
& modFitgbmcv$results$interaction.depth == modFitgbmcv$bestTune$interaction.depth & modFitgbmcv$results$shrinkage == modFitgbmcv$bestTune$shrinkage, ]

optaccugbmcv <- optrowgbmcv[, "Accuracy"]

```

This leads to an in-sample accuracy of `r round(optaccugbmcv,3)` for the preferred model.

```{r, echo=FALSE , warning=FALSE , message=FALSE}
modFitgbmcv$finalModel
modFitgbmcv
```



```{r, echo=FALSE , warning=FALSE, message=FALSE}
plot(modFitgbmcv)

```



```{r, echo = FALSE, warning=FALSE , message=FALSE}
predgbmcv <- predict(modFitgbmcv,valid) 
predgbm <- predict(modFitgbm,valid)

gbmboottrue <- valid$classe == predgbm
gbmboottrue_valid <- sum(gbmboottrue)/length(gbmboottrue)
```


```{r, echo = FALSE, warning=FALSE , message=FALSE}
gbmcvboottrue <- valid$classe == predgbmcv
gbmcvboottrue_valid <- sum(gbmcvboottrue)/length(gbmcvboottrue)
```

The out-of-sample accuracy (in the validation set) for the bootstrapped model (`r round( gbmboottrue_valid  ,4)`) is sligtly higher than for the K-fold cross-validated model (`r round( gbmcvboottrue_valid  ,4)`). This is substantially better than the linear discriminant model, but we shall show below it is possible to even further improve on this. 


##Random forest models


We have then fitted a random forest with the default parameters from  the "caret" package: 

```{r, echo=FALSE , warning=FALSE, message=FALSE}
rffirstrun <- FALSE

if(rffirstrun == TRUE){
  modFitRF <- train(classe  ~ .  , data = train, method="rf",
                    preProcess=c("center","scale")
                    )
  save(modFitRF, file = "modFitRF.RData")  
} else {
  load("modFitRF.RData")
}
```

The model has been bootstrapped for the purpose of cross validation with `r modFitRF$control$number  ` resamples and the accuracy as performance metric.  In the preferred model, `r round(modFitRF$results[modFitRF$results$mtry == modFitRF$bestTune$mtry,"mtry"], 3)` features are selected randomly at each split. This leads to an in-sample accuracy of `r round(modFitRF$results[modFitRF$results$mtry == modFitRF$bestTune$mtry,"Accuracy"], 3)`.

```{r, echo=FALSE , warning=FALSE, message=FALSE}
modFitRF
#importance(modFitRF)
trellis.par.set(caretTheme())
#plot(modFitRF)
```


The final model results in a collection of `r modFitRF$finalModel$ntree` trees. The 5 most important features are: 

```{r, echo=FALSE , warning=FALSE, message=FALSE}
head(modFitRF$finalModel$importance[order(modFitRF$finalModel$importance[,"MeanDecreaseGini"], decreasing = TRUE) , ])
```



```{r, echo=FALSE , warning=FALSE, message=FALSE}
rfkfoldfirstrun <- FALSE

if(rfkfoldfirstrun == TRUE){
  rfctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
  
  modFitRFkfold <- train(classe  ~ .  , data = train, method="rf", trControl = rfctrl,
                    preProcess=c("center","scale")
                    )
  save(modFitRFkfold, file = "modFitRFkfold.RData")  
} else {
  load("modFitRFkfold.RData")
}
```

We have also fitted a second random forest, but instead of bootstrap resampling, we used K-fold cross-validation, with K = 10. 
In the preferred model, `r round(modFitRFkfold$results[modFitRFkfold$results$mtry == modFitRFkfold$bestTune$mtry,"mtry"], 3)` features are selected randomly at each split. This leads to an in-sample accuracy of `r round(modFitRFkfold$results[modFitRFkfold$results$mtry == modFitRFkfold$bestTune$mtry,"Accuracy"], 3)`.

```{r, echo=FALSE , warning=FALSE, message=FALSE}
modFitRFkfold
trellis.par.set(caretTheme())
#plot(modFitRFkfold)
```

The 5 most important features are: 

```{r, echo=FALSE , warning=FALSE, message=FALSE}
head(modFitRFkfold$finalModel$importance[order(modFitRFkfold$finalModel$importance[,"MeanDecreaseGini"], decreasing = TRUE) , ])
```

Note that that these are equal to the 5 most important features in the bootstrapped model, albeit in a slightly different order. 


Both "random forest" models result in identical predictions in the validation data set. 
Therefore, stacking these models will not improve performance. 



```{r, echo = TRUE, warning=FALSE , message=FALSE}
predRF <- predict(modFitRF,valid)
predRFk <- predict(modFitRFkfold,valid)
all.equal(predRF,predRFk)
```


```{r, echo = FALSE, warning=FALSE , message=FALSE}
rfboottrue <- valid$classe == predRF
rf_valid <- sum(rfboottrue)/length(rfboottrue)
```

Moreover, the "random forest" model perfectly predicts all values in the validation set: 


```{r, warning=FALSE , message=FALSE}
all.equal(valid$classe, predRF)
```


#Conclusion

We have tested several predictive models to investigate "how (well)" a weight lifting exercise was performed by the wearer.


Tree based models clearly outperform linear discriminant analysis. "Random forest" model perform best, both inside and outside the sample (with a 100% accuracy for the current validation data set). Taking into account its shorter running time, we prefer a bootstrapped model to the k-fold cross-validated model. 

It should however be noted that the "boosted trees" model also perform extremely well, both inside and outside the sample.  


#Reference

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.






